# Malware URL Lookup Coding Exercise


## Problem Statement

We have an HTTP proxy that is scanning traffic, looking for malware URLs. Before allowing HTTP connections to be made, this proxy asks a service that maintains several databases of malware URLs if the resource being requested is known to contain malware.

Write a small web service that responds to GET requests where the caller passes in a URL and the service responds with some information about that URL. The GET requests would look like this:

```GET /v1/urlinfo/{resource_url_with_query_string}```

The caller wants to know if it is safe to access that URL or not.

## API Schema

### Check if URL is a known source of malware

```
GET /v1/urlinfo/{resource_url_with_query_string}
```

Returns an application/json reponse object indicating whether or not the URL is "safe".

```json
    {
        "safe": true
    }
```

### Submit URL(s) that are a known source of malware

```
POST /v1/urlinfo/
```

Accepts URLs to be added to the malware database. Expects them as an application/json object in the body of a POST request (as defined below)

```json
    {
        "urls": [
            "url1.com",
            "url2.com",
            ...
            "urln.com"
        ]
    }
```


## Bonus Features

> The size of the URL list could grow infinitely, how might you scale this beyond the memory capacity of the system? Bonus if you implement this.

There are two dimensions to this questions, the first being reducing the storage footprint/memory requirements of the system, and the second being: given the system is optimized, how does it scale?

For the former, I have considered it may use less storage to only store hashes of known malware sources, thereby eliminating the need to store valid URLs, and also storing fewer bytes per entry, since a hash of the value will typically be smaller than the original URL. I will circle back and look into this if I have time, since I would need to look into the frequency of hash collisions, size of hashes compared to average URL length, etc.

For the latter, my choice would be to use a distributed cloud-based database, which would scale automatically given the needs of the system. Again, I will tackle this if I have time.

> The number of requests may exceed the capacity of this system, how might you solve that? Bonus if you implement this.

This question is similar to the last, in that we should examine the problem from the same 2 angles. Firstly, for the system to function at optimal capacity, we would need it to handle requests conncurrently (i.e. non-blocking) so that it may accept incoming requests while it's still processing prior requests. 

Secondly, given the system is already optimized, my answer would be to load balance between distributed instances of the application runnning on different devices, for example Docker containers orchestrated by Kubernetes.

> What are some strategies you might use to update the service with new URLs? Updates may be as many as 5000 URLs a day with updates arriving every 10 minutes.

In my opinion, the best way to handle these variably-lengthed, variably-timed updates would be to receive them via API. **For the sake of the example, I will assume they come from a trusted, vetted source**. To mamiximize the system's optimization, this "Update API" could be decoupled from the previously defined "Lookup API" to allow them to scale independently, and without resource competition.
