# Malware URL Lookup Coding Exercise


## Problem Statement

We have an HTTP proxy that is scanning traffic, looking for malware URLs. Before allowing HTTP connections to be made, this proxy asks a service that maintains several databases of malware URLs if the resource being requested is known to contain malware.

Write a small web service that responds to GET requests where the caller passes in a URL and the service responds with some information about that URL. The GET requests would look like this:

```GET /v1/urlinfo/{resource_url_with_query_string}```

The caller wants to know if it is safe to access that URL or not.

## API Schema

```
GET /v1/urlinfo/{resource_url_with_query_string}
```

Returns an application/json reponse object indicating whether or not the URL is "safe".

```json
    {
        "safe": true
    }
```

```
POST /v1/urlinfo/
``` 

Accepts URLs to be added to the malware database. Expects them as an application/json object in the body of a POST request (as defined below)

```json
    {
        "urls": [
            "url1.com",
            "url2.com",
            ...
            "urln.com"
        ]
    }
```


## Bonus Features

> The size of the URL list could grow infinitely, how might you scale this beyond the memory capacity of the system? Bonus if you implement this.

There are two dimensions to this questions, the first being reducing the storage footprint/memory requirements of the syustem, and the second being geiven the system is optuimized, how does it scale.

For the former, I have considered it may use less storage to only store hashes of known malware sources, thereby eliminating the need to store valid URLs, and also storing fewer bytes per entry, since a hash of the value will typically be smaller than the original URL. I will circle back and look into this if I have time, since I would need to look into the frequency of hash collisions, etc.

For the latter, my choice would be to use a distributed cloud-based database, which would scale automatically given the needs of the system. Again, I will tackle this if I have time.

> The number of requests may exceed the capacity of this system, how might you solve that? Bonus if you implement this.

This question is similar to the last, in that we should examine it with the same lens. Firstly, for the system to function at optimal capacity, we would need it to run conncurrently (non-blocking) so that it may accept incoming request while processing prior and incomplete requests. 

Secondly, given the system is already optimized, my answer would be to load balance between distributed instances of the application runnning on different devices, orchestrated by Kubernetes for example.

> What are some strategies you might use to update the service with new URLs? Updates may be as many as 5000 URLs a day with updates arriving every 10 minutes.

In my opinion, the best way to handle these variably-lengthed, variably-timed updates would be to receive them via API, *assuming for the sake of the example they come from a trusted vetted source*. To mamiximize the system's optimization, this "Update API" could be decoupled from the previously defined "Lookup API" to allow them to scale independently, and without resource competition.